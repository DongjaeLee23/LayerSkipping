{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/dlee906/.conda/envs/layer_skip_1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaConfig, LlamaTokenizer, GenerationConfig\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [01:06<00:00, 22.22s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.69it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"typing import List\\ndef bucket_sort(A: List):\"\n",
    "\n",
    "checkpoint = \"facebook/layerskip-llama2-7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "max_new_tokens = 512\n",
    "do_sample = True\n",
    "top_p = 0.9\n",
    "temperature = 0.6\n",
    "\n",
    "warmup = 2\n",
    "repeat = 10\n",
    "\n",
    "# Range for dynamic early exit\n",
    "min_exit = 2  # <-- minimum layer to exit\n",
    "max_exit = 12  # <-- maximum layer to exit\n",
    "\n",
    "config = LlamaConfig.from_pretrained(checkpoint)\n",
    "model = LlamaForCausalLM.from_pretrained(checkpoint, config=config, torch_dtype=torch.float16)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(checkpoint, use_fast=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "    \"do_sample\": do_sample,\n",
    "    \"top_p\": top_p, \n",
    "    \"temperature\": temperature,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dynamic_assistant_model(base_model, exit_layer):\n",
    "    weights_memo = {id(w): w for w in base_model.parameters()}\n",
    "    assistant_model = deepcopy(base_model, memo=weights_memo)\n",
    "    assistant_model.model.layers = assistant_model.model.layers[:exit_layer]\n",
    "    del assistant_model.model.layers[exit_layer:]\n",
    "    return assistant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "100%|██████████| 2/2 [01:01<00:00, 30.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoregressive Decoding (no early exit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:50<00:00, 11.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typing import List\n",
      "def bucket_sort(A: List):\n",
      "    \"\"\"\n",
      "    Sorts a list using bucket sort.\n",
      "    \"\"\"\n",
      "    # initialize bucket array\n",
      "    buckets = [0] * len(A)\n",
      "    # initialize index\n",
      "    index = 0\n",
      "    # initialize max\n",
      "    max = 0\n",
      "    # loop through array\n",
      "    for i in range(len(A)):\n",
      "        # if value is larger than max\n",
      "        if A[i] > max:\n",
      "            # set max to value\n",
      "            max = A[i]\n",
      "            # increment index\n",
      "            index += 1\n",
      "        # set bucket value\n",
      "        buckets[index] = A[i]\n",
      "    # sort buckets\n",
      "    for i in range(len(buckets)):\n",
      "        # if bucket value is greater than max\n",
      "        if buckets[i] > max:\n",
      "            # swap max and bucket\n",
      "            max = buckets[i]\n",
      "            buckets[i] = max\n",
      "    # return sorted array\n",
      "    return buckets\n",
      "\n",
      "\n",
      "# test code\n",
      "\n",
      "\n",
      "# test case 1\n",
      "A = [4, 3, 1, 2, 5]\n",
      "print(bucket_sort(A))\n",
      "\n",
      "\n",
      "# test case 2\n",
      "A = [1, 2, 3, 4, 5, 6]\n",
      "print(bucket_sort(A))\n",
      "\n",
      "\n",
      "\t=========================\n",
      "\tAverage Generation Time: 11.03 s\n",
      "\tAverage Tokens per Second: 31.12 tokens per sec\n",
      "\n",
      "\n",
      "Self-Speculative Decoding (dynamic early exit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:46<00:00, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typing import List\n",
      "def bucket_sort(A: List):\n",
      "    \"\"\"\n",
      "    Bucket sort is a sorting algorithm that divides the input list into a number of buckets, sorts each bucket, and then merges the sorted buckets.\n",
      "    The input list is split into a number of buckets, each bucket is sorted, and the sorted buckets are merged together to form the output.\n",
      "    :param A: List\n",
      "    :return: List\n",
      "    \"\"\"\n",
      "    # bucket_size = int(len(A)/2)\n",
      "    bucket_size = 1\n",
      "    buckets = []\n",
      "    for i in range(len(A)):\n",
      "        if i%bucket_size == 0:\n",
      "            buckets.append([])\n",
      "        buckets[-1].append(A[i])\n",
      "    for i in range(len(buckets)-1):\n",
      "        buckets[i+1] = sorted(buckets[i])\n",
      "    buckets[-1] = sorted(buckets[-1])\n",
      "    return buckets\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    A = [3, 1, 2, 5, 4, 7, 6, 9, 8]\n",
      "    B = bucket_sort(A)\n",
      "    print(B)\n",
      "\n",
      "\n",
      "\t=========================\n",
      "\tAverage Generation Time: 10.62 s\n",
      "\tAverage Tokens per Second: 36.27 tokens per sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Warmup\n",
    "print(\"Warmup\")\n",
    "for i in tqdm(range(warmup)):\n",
    "    _ = model.generate(**inputs, **generation_config)\n",
    "    early_exit = random.randint(min_exit, max_exit)  # Random early exit\n",
    "    assistant_model = create_dynamic_assistant_model(model, early_exit).to(device)\n",
    "    _ = model.generate(**inputs, **generation_config, assistant_model=assistant_model)\n",
    "\n",
    "print(\"Autoregressive Decoding (no early exit)\")\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "for i in tqdm(range(repeat)):\n",
    "    start = time()\n",
    "    outputs = model.generate(**inputs, **generation_config)\n",
    "    total_time += time() - start\n",
    "    total_tokens += outputs.numel()\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "print(\"\\n\\t=========================\")\n",
    "print(f\"\\tAverage Generation Time: {total_time / repeat:.2f} s\")\n",
    "print(f\"\\tAverage Tokens per Second: {total_tokens / total_time:.2f} tokens per sec\\n\\n\")\n",
    "\n",
    "print(\"Self-Speculative Decoding (dynamic early exit)\")\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "for i in tqdm(range(repeat)):\n",
    "    early_exit = random.randint(min_exit, max_exit)  # <-- Dynamic early exit for each generation\n",
    "    assistant_model = create_dynamic_assistant_model(model, early_exit).to(device)\n",
    "    start = time()\n",
    "    outputs = model.generate(**inputs, **generation_config, assistant_model=assistant_model)\n",
    "    total_time += time() - start\n",
    "    total_tokens += outputs.numel()\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "print(\"\\n\\t=========================\")\n",
    "print(f\"\\tAverage Generation Time: {total_time / repeat:.2f} s\")\n",
    "print(f\"\\tAverage Tokens per Second: {total_tokens / total_time:.2f} tokens per sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layer_skip_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
