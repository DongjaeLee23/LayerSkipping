{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/dlee906/.conda/envs/layer_skip_1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-2.7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/layerskip-llama2-7B\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping layers: [2, 3, 4, 5, 7, 9, 10, 11, 12, 14, 18, 20, 21, 23, 27, 28]\n",
      "Pruned model saved to ./pruned_layerskip\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Function for pruning the model\n",
    "def prune_model(model_name, prune_ratio, save_path):\n",
    "    \"\"\"\n",
    "    Prune the model layers and save the pruned model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: str, name or path of the model to prune (e.g., \"facebook/layerskip-llama2-7B\")\n",
    "    - prune_ratio: float, the percentage of layers to keep (e.g., 0.5 for 50% of layers)\n",
    "    - save_path: str, directory to save the pruned model\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Get the number of layers and prune\n",
    "    num_layers = len(model.model.layers)\n",
    "    keep_layers = sorted(torch.randperm(num_layers)[:int(num_layers * (1 - prune_ratio))].tolist())\n",
    "    \n",
    "    print(f\"Keeping layers: {keep_layers}\")\n",
    "\n",
    "    # Prune layers\n",
    "    model.model.layers = torch.nn.ModuleList([model.model.layers[i] for i in keep_layers])\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(save_path)\n",
    "    print(f\"Pruned model saved to {save_path}\")\n",
    "\n",
    "# Example usage - you can run this cell directly in Jupyter\n",
    "model_name = \"facebook/layerskip-llama2-7B\"\n",
    "prune_ratio = 0.5  # 50% pruning\n",
    "save_path = \"./pruned_layerskip\"\n",
    "\n",
    "# Run the pruning\n",
    "prune_model(model_name, prune_ratio, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mipywidgets\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mwidgets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m display\n\u001b[1;32m      4\u001b[0m prune_ratio_slider \u001b[39m=\u001b[39m widgets\u001b[39m.\u001b[39mFloatSlider(value\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPrune Ratio:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "prune_ratio_slider = widgets.FloatSlider(value=0.5, min=0.1, max=0.9, step=0.1, description='Prune Ratio:')\n",
    "display(prune_ratio_slider)\n",
    "\n",
    "# Later in the cell:\n",
    "prune_model(\"facebook/layerskip-llama2-7B\", prune_ratio_slider.value, \"./pruned_layerskip\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layer_skip_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
